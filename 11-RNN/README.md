# ‚è∞ R√©seaux de Neurones R√©currents (RNN) - Pr√©diction du Futur

## üéØ **CONTEXTE ET POUVOIR DE LA PR√âDICTION TEMPORELLE**

**Qu'est-ce que les RNN et pourquoi c'est le graal de l'analyse de donn√©es ?**

Les RNN (Recurrent Neural Networks) sont **la technologie qui permet de pr√©dire l'avenir** √† partir du pass√©. Dans un monde o√π **le temps est la dimension la plus importante**, savoir anticiper les tendances est **une comp√©tence superpuissance**.

**Contexte du probl√®me** :
- **Donn√©es temporelles partout** : Prix des actions, ventes, temp√©rature, trafic, sant√©...
- **Besoin de pr√©diction** : Anticiper pour optimiser, pr√©venir, planifier
- **Complexit√© temporelle** : Les patterns changent dans le temps
- **D√©pendances s√©quentielles** : Le futur d√©pend du pass√©

**Pourquoi les RNN sont-ils r√©volutionnaires ?**
- **M√©moire** : Se souviennent des informations pass√©es
- **S√©quentiel** : Traitent les donn√©es dans l'ordre chronologique
- **Pr√©diction** : Anticipent les valeurs futures
- **Adaptabilit√©** : Apprennent des patterns temporels complexes
- **G√©n√©ralisation** : Appliquent ce qu'ils apprennent √† de nouvelles s√©quences

**Applications qui changent le monde** :
- **Finance** : Pr√©diction des prix, trading algorithmique, gestion des risques
- **M√©t√©o** : Pr√©visions m√©t√©orologiques, mod√©lisation climatique
- **M√©decine** : Pr√©diction d'√©pid√©mies, monitoring de patients
- **Transport** : Optimisation du trafic, maintenance pr√©dictive
- **√ânergie** : Pr√©diction de consommation, optimisation des r√©seaux
- **E-commerce** : Pr√©diction de ventes, gestion des stocks

## üìö **CONTENU DU MODULE**

### Notebooks d'Apprentissage Progressifs
- **00-RNN-Exemple-Sinuso√Ødale.ipynb** - **Les bases** : Introduction avec une onde sinuso√Ødale simple
- **01-RNN-Exemple-Time-Series.ipynb** - **S√©ries r√©elles** : Analyse de s√©ries temporelles complexes
- **02-RNN-Exercice.ipynb** - **Pratique** : Exercices pour ma√Ætriser les RNN
- **04-BONUS-RNN-Multivari√©.ipynb** - **Avanc√©** : RNN multivari√©s pour des donn√©es complexes

### Datasets d'Exemple Temporels
- `energydata_complete.csv` - **Donn√©es √©nerg√©tiques** : Consommation d'√©nergie dans le temps
- `Frozen_Dessert_Production.csv` - **Production industrielle** : Production de desserts glac√©s
- `MRTSSM448USN.csv` - **Donn√©es √©conomiques** : Ventes au d√©tail
- `RSCCASN.csv` - **S√©rie temporelle** : Donn√©es de vente au d√©tail
- `RSCCASN2021.csv` - **Donn√©es 2021** : √âvolution r√©cente
- `RSCCASN2023.csv` - **Donn√©es 2023** : Tendances actuelles

## üéØ **OBJECTIFS D'APPRENTISSAGE**

**√Ä la fin de ce module, vous saurez :**
- **Pourquoi** les RNN sont parfaits pour les donn√©es temporelles
- **Comment** pr√©dire l'avenir √† partir de donn√©es historiques
- **Comment** choisir entre Simple RNN, LSTM, et GRU
- **Comment** pr√©processer des donn√©es temporelles
- **Comment** √©valuer la qualit√© de vos pr√©dictions

## üöÄ Pr√©requis

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
```

## üìñ Concepts Cl√©s

### Architecture RNN de Base

```python
# Mod√®le RNN simple
model = keras.Sequential([
    layers.SimpleRNN(50, return_sequences=True, input_shape=(timesteps, features)),
    layers.SimpleRNN(50, return_sequences=False),
    layers.Dense(25),
    layers.Dense(1)
])
```

### Types de RNN

#### Simple RNN
```python
layers.SimpleRNN(units=50, activation='tanh')
```

#### LSTM (Long Short-Term Memory)
```python
layers.LSTM(units=50, return_sequences=True)
```

#### GRU (Gated Recurrent Unit)
```python
layers.GRU(units=50, return_sequences=True)
```

## üîß Exemples Pratiques

### Pr√©diction de S√©rie Temporelle Simple
```python
# Pr√©paration des donn√©es
def create_sequences(data, timesteps):
    X, y = [], []
    for i in range(timesteps, len(data)):
        X.append(data[i-timesteps:i])
        y.append(data[i])
    return np.array(X), np.array(y)

# Cr√©ation des s√©quences
timesteps = 60
X, y = create_sequences(scaled_data, timesteps)

# Division train/test
split = int(0.8 * len(X))
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# Mod√®le LSTM
model = keras.Sequential([
    layers.LSTM(50, return_sequences=True, input_shape=(timesteps, 1)),
    layers.Dropout(0.2),
    layers.LSTM(50, return_sequences=True),
    layers.Dropout(0.2),
    layers.LSTM(50),
    layers.Dropout(0.2),
    layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')
```

### Pr√©diction Multivari√©e
```python
# Mod√®le pour donn√©es multivari√©es
model = keras.Sequential([
    layers.LSTM(50, return_sequences=True, input_shape=(timesteps, n_features)),
    layers.Dropout(0.2),
    layers.LSTM(50, return_sequences=False),
    layers.Dropout(0.2),
    layers.Dense(25),
    layers.Dense(1)
])
```

## üìä Visualisation des R√©sultats

```python
# Pr√©dictions vs Valeurs r√©elles
plt.figure(figsize=(15, 6))
plt.plot(y_test, label='Valeurs R√©elles', alpha=0.7)
plt.plot(predictions, label='Pr√©dictions', alpha=0.7)
plt.title('Pr√©diction de S√©rie Temporelle')
plt.xlabel('Temps')
plt.ylabel('Valeur')
plt.legend()
plt.show()
```

## üí° Conseils d'Apprentissage

1. **Comprenez la s√©quence** - Les RNN sont con√ßus pour les donn√©es temporelles
2. **Choisissez le bon type** - LSTM pour la m√©moire longue, GRU pour la simplicit√©
3. **Normalisez les donn√©es** - Crucial pour la convergence
4. **Exp√©rimentez les timesteps** - Plus long ‚â† toujours mieux
5. **Utilisez la r√©gularisation** - Dropout pour √©viter l'overfitting

## üîó Ressources Suppl√©mentaires

- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Deep Learning for Time Series](https://www.manning.com/books/deep-learning-for-time-series-forecasting)
- [TensorFlow RNN Guide](https://www.tensorflow.org/guide/keras/rnn)
