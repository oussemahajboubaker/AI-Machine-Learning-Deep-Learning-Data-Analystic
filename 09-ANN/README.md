# üß† R√©seaux de Neurones Artificiels (ANN) - L'Intelligence Artificielle

## üéØ **CONTEXTE ET R√âVOLUTION DU DEEP LEARNING**

**Qu'est-ce que le Deep Learning et pourquoi c'est r√©volutionnaire ?**

Les R√©seaux de Neurones Artificiels (ANN) sont **l'√©pine dorsale de l'Intelligence Artificielle moderne**. Inspir√©s du fonctionnement du cerveau humain, ils peuvent apprendre des patterns complexes que les algorithmes traditionnels ne peuvent pas d√©tecter.

**Contexte historique** :
- **1940s** : Premiers mod√®les de neurones artificiels
- **1980s** : Perceptron et premiers r√©seaux multicouches
- **2000s** : R√©volution avec le deep learning et les GPU
- **2010s** : Explosion avec TensorFlow, PyTorch, et les donn√©es massives
- **Aujourd'hui** : L'IA est partout - reconnaissance d'images, traduction, voitures autonomes...

**Pourquoi le Deep Learning est-il r√©volutionnaire ?**
- **Apprentissage automatique** : D√©couvre des patterns sans programmation explicite
- **Capacit√© d'abstraction** : Comprend des concepts complexes
- **G√©n√©ralisation** : Applique ce qu'il apprend √† de nouvelles situations
- **Performance** : Surpasse les humains dans de nombreux domaines
- **√âvolutivit√©** : Plus de donn√©es = meilleures performances

**Applications qui changent le monde** :
- **Reconnaissance d'images** : Photos, vid√©os, imagerie m√©dicale
- **Traitement du langage** : Traduction, chatbots, assistants vocaux
- **Voitures autonomes** : Reconnaissance d'objets, pr√©diction de trajectoires
- **M√©decine** : Diagnostic, d√©couverte de m√©dicaments, imagerie
- **Finance** : Trading algorithmique, d√©tection de fraude
- **Jeux** : AlphaGo, jeux vid√©o intelligents

## üìö **CONTENU DU MODULE**

### Notebooks d'Apprentissage Progressifs
- **00-Bases-Syntaxe-Keras.ipynb** - **Les fondations** : Syntaxe Keras et TensorFlow 2.x
- **01-R√©gression-Keras.ipynb** - **Pr√©diction continue** : R√©seaux pour la r√©gression
- **02-Classification-Keras.ipynb** - **Classification intelligente** : R√©seaux pour la classification
- **03-Exercice-Projet-Keras.ipynb** - **Projet pratique** : Application compl√®te
- **05-Tensorboard.ipynb** - **Monitoring** : Visualisation et suivi des performances

### Datasets d'Exemple R√©els
- `cancer_classification.csv` - **Donn√©es m√©dicales** : Classification de cancer
- `fake_reg.csv` - **Donn√©es simul√©es** : R√©gression simple pour apprendre
- `kc_house_data.csv` - **Donn√©es immobili√®res** : Pr√©diction de prix de maisons
- `lending_club_info.csv` - **Donn√©es financi√®res** : Informations sur les pr√™ts
- `lending_club_loan_two.csv` - **Donn√©es de pr√™ts** : Classification de risque

## üéØ **OBJECTIFS D'APPRENTISSAGE**

**√Ä la fin de ce module, vous saurez :**
- **Pourquoi** le deep learning est la technologie la plus importante de notre √©poque
- **Comment** construire des r√©seaux de neurones efficaces
- **Comment** choisir l'architecture appropri√©e pour votre probl√®me
- **Comment** √©viter l'overfitting et optimiser les performances
- **Comment** utiliser TensorBoard pour monitorer vos mod√®les

## üöÄ Pr√©requis

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
```

## üìñ Concepts Cl√©s

### Architecture d'un R√©seau de Neurones

```python
# Cr√©ation d'un mod√®le s√©quentiel
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(1, activation='sigmoid')  # Classification binaire
])

# Compilation du mod√®le
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)
```

### Types de Couches

#### Couches Denses (Fully Connected)
```python
layers.Dense(units=64, activation='relu')
```

#### Couches de Dropout (R√©gularisation)
```python
layers.Dropout(rate=0.2)
```

#### Couches de Normalisation
```python
layers.BatchNormalization()
```

### Fonctions d'Activation

```python
# Fonctions d'activation courantes
'relu'      # Rectified Linear Unit
'sigmoid'   # Pour la classification binaire
'softmax'   # Pour la classification multi-classe
'tanh'      # Tangente hyperbolique
'linear'    # Pas d'activation (r√©gression)
```

### Optimiseurs

```python
# Optimiseurs disponibles
'adam'      # Adaptive Moment Estimation (recommand√©)
'sgd'       # Stochastic Gradient Descent
'rmsprop'   # Root Mean Square Propagation
'adagrad'   # Adaptive Gradient
```

## üîß Exemples Pratiques

### R√©gression avec Keras
```python
# Pr√©paration des donn√©es
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Mod√®le de r√©gression
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dropout(0.2),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(1)  # Pas d'activation pour la r√©gression
])

model.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae']
)

# Entra√Ænement
history = model.fit(
    X_train_scaled, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)
```

### Classification avec Keras
```python
# Mod√®le de classification
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(1, activation='sigmoid')  # Classification binaire
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Entra√Ænement
history = model.fit(
    X_train_scaled, y_train,
    epochs=50,
    batch_size=32,
    validation_data=(X_test_scaled, y_test),
    callbacks=[keras.callbacks.EarlyStopping(patience=5)]
)
```

### Visualisation des R√©sultats
```python
# Historique d'entra√Ænement
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()
```

## üéõÔ∏è Techniques Avanc√©es

### Callbacks
```python
# Callbacks utiles
callbacks = [
    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5),
    keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True)
]
```

### R√©gularisation
```python
# L1 et L2 regularization
layers.Dense(64, activation='relu', 
             kernel_regularizer=keras.regularizers.l2(0.01))

# Dropout
layers.Dropout(0.3)

# Batch Normalization
layers.BatchNormalization()
```

### TensorBoard
```python
# Configuration TensorBoard
tensorboard_callback = keras.callbacks.TensorBoard(
    log_dir='./logs',
    histogram_freq=1,
    write_graph=True,
    write_images=True
)

# Utilisation dans l'entra√Ænement
model.fit(X_train, y_train, callbacks=[tensorboard_callback])
```

## üí° Conseils d'Apprentissage

1. **Commencez simple** - Mod√®les simples avant les complexes
2. **Normalisez vos donn√©es** - Crucial pour la convergence
3. **Utilisez la validation** - √âvitez l'overfitting
4. **Exp√©rimentez** - Testez diff√©rentes architectures
5. **Monitorer** - Utilisez TensorBoard pour visualiser
6. **R√©gularisez** - Dropout et Batch Normalization

## üîó Ressources Suppl√©mentaires

- [TensorFlow Documentation](https://www.tensorflow.org/)
- [Keras Documentation](https://keras.io/)
- [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)
- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
