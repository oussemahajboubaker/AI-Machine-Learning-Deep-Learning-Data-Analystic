# üéØ Naive Bayes - Classification Intelligente de Texte

## üéØ **CONTEXTE ET R√âVOLUTION DE LA CLASSIFICATION DE TEXTE**

**Qu'est-ce que Naive Bayes et pourquoi c'est r√©volutionnaire pour le texte ?**

Naive Bayes est **l'algorithme de classification probabiliste** le plus efficace pour analyser du texte. Dans notre √®re digitale o√π **des millions de textes** sont g√©n√©r√©s chaque seconde, savoir les analyser automatiquement est **une comp√©tence superpuissance**.

**Contexte du probl√®me** :
- **Explosion du contenu textuel** : Emails, tweets, avis clients, articles, messages...
- **Besoin d'automatisation** : Impossible de lire manuellement des millions de textes
- **Classification cruciale** : Spam vs important, positif vs n√©gatif, urgent vs normal
- **D√©couverte d'insights** : Comprendre les sentiments et tendances des clients

**Pourquoi Naive Bayes est-il r√©volutionnaire ?**
- **Simplicit√©** : Facile √† comprendre et impl√©menter
- **Efficacit√©** : Tr√®s rapide m√™me sur de gros volumes
- **Performance** : Excellent pour la classification de texte
- **Probabilit√©s** : Fournit des scores de confiance
- **Robustesse** : Fonctionne bien m√™me avec peu de donn√©es

**Applications qui changent le monde** :
- **Filtrage de spam** : Gmail, Outlook (milliards d'emails filtr√©s)
- **Analyse de sentiment** : Comprendre l'opinion des clients
- **Chatbots** : Classifier les intentions des utilisateurs
- **R√©seaux sociaux** : Mod√©ration automatique de contenu
- **E-commerce** : Recommandations bas√©es sur les avis

## üìö **CONTENU DU MODULE**

### Notebooks d'Apprentissage
- **Naive bayes.ipynb** - **Introduction compl√®te** : De la th√©orie √† la pratique

### Datasets d'Exemple R√©els
- `smsspamcollection/SMSSpamCollection` - **Collection SMS** : Messages spam vs l√©gitimes

### D√©p√¥t des Exercices
Le dossier `ESPACE DEPOT` contient les exercices soumis par les √©tudiants.

### Ressources Visuelles P√©dagogiques
- `bayes_formula.png` - **Formule de Bayes** : La base math√©matique
- `countvectorizer.png` - **CountVectorizer** : Comment transformer le texte en nombres
- `dqnb.png` - **Diagramme Naive Bayes** : Visualisation de l'algorithme
- `naivebayes.png` - **Repr√©sentation visuelle** : Comprendre le processus
- `tfidf.png` - **TF-IDF** : Pond√©ration des mots importants

## üéØ **OBJECTIFS D'APPRENTISSAGE**

**√Ä la fin de ce module, vous saurez :**
- **Pourquoi** Naive Bayes est parfait pour l'analyse de texte
- **Comment** transformer du texte en donn√©es num√©riques
- **Comment** classifier automatiquement des milliers de documents
- **Comment** analyser les sentiments des clients
- **Comment** cr√©er un filtre anti-spam intelligent

## üöÄ Pr√©requis

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
```

## üìñ Concepts Cl√©s

### Th√©or√®me de Bayes
```
P(A|B) = P(B|A) * P(A) / P(B)
```

O√π :
- P(A|B) = Probabilit√© a posteriori
- P(B|A) = Vraisemblance
- P(A) = Probabilit√© a priori
- P(B) = Probabilit√© marginale

### Naive Bayes
L'algorithme Naive Bayes suppose que les features sont conditionnellement ind√©pendants :

```
P(y|x1,x2,...,xn) = P(y) * ‚àè P(xi|y)
```

### Types de Naive Bayes

#### Multinomial Naive Bayes
```python
# Pour les donn√©es de comptage (texte)
from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

#### Gaussian Naive Bayes
```python
# Pour les donn√©es continues
from sklearn.naive_bayes import GaussianNB

model = GaussianNB()
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

## üî§ Pr√©traitement de Texte

### Tokenization et Nettoyage
```python
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

def preprocess_text(text):
    # Convertir en minuscules
    text = text.lower()
    
    # Supprimer la ponctuation et les chiffres
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    
    # Tokenization
    tokens = word_tokenize(text)
    
    # Supprimer les stop words
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    
    return ' '.join(tokens)
```

### Vectorisation de Texte

#### Count Vectorizer
```python
from sklearn.feature_extraction.text import CountVectorizer

# Cr√©er le vectorizer
cv = CountVectorizer(max_features=5000, stop_words='english')

# Transformer les donn√©es
X = cv.fit_transform(text_data)
feature_names = cv.get_feature_names_out()
```

#### TF-IDF Vectorizer
```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Cr√©er le vectorizer TF-IDF
tfidf = TfidfVectorizer(max_features=5000, stop_words='english')

# Transformer les donn√©es
X = tfidf.fit_transform(text_data)
```

## üìä Classification de Sentiment

### Exemple Complet
```python
# Chargement des donn√©es
df = pd.read_csv('SMSSpamCollection', sep='\t', names=['label', 'message'])

# Pr√©traitement
df['processed_text'] = df['message'].apply(preprocess_text)

# Vectorisation
vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['processed_text'])
y = df['label']

# Division train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entra√Ænement du mod√®le
model = MultinomialNB()
model.fit(X_train, y_train)

# Pr√©dictions
y_pred = model.predict(X_test)

# √âvaluation
print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
```

### Visualisation des R√©sultats
```python
# Matrice de confusion
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
plt.title('Matrice de Confusion')
plt.xlabel('Pr√©dictions')
plt.ylabel('Valeurs R√©elles')
plt.show()

# Importance des mots
feature_names = vectorizer.get_feature_names_out()
feature_importance = model.feature_log_prob_[1] - model.feature_log_prob_[0]
top_words = np.argsort(feature_importance)[-20:]

plt.figure(figsize=(10, 8))
plt.barh(range(len(top_words)), feature_importance[top_words])
plt.yticks(range(len(top_words)), [feature_names[i] for i in top_words])
plt.title('Mots les Plus Importants pour la Classification')
plt.xlabel('Importance')
plt.show()
```

## üí° Avantages et Inconv√©nients

### Avantages
- **Simplicit√©** - Facile √† impl√©menter et comprendre
- **Rapidit√©** - Entra√Ænement et pr√©diction tr√®s rapides
- **Efficacit√©** - Fonctionne bien avec peu de donn√©es
- **Probabilit√©s** - Fournit des probabilit√©s de classe

### Inconv√©nients
- **Hypoth√®se na√Øve** - L'ind√©pendance des features est rarement vraie
- **Sensibilit√©** - Peut √™tre sensible aux features corr√©l√©es
- **Qualit√© des donn√©es** - N√©cessite un bon pr√©traitement

## üéØ Applications Pratiques

- **Filtrage de spam** - Classification des emails
- **Analyse de sentiment** - Opinion sur les produits/services
- **Classification de documents** - Cat√©gorisation automatique
- **D√©tection de fraude** - Identification des transactions suspectes

## üîó Ressources Suppl√©mentaires

- [Scikit-learn Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)
- [NLTK Documentation](https://www.nltk.org/)
- [Text Classification with Naive Bayes](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/)
- [Natural Language Processing with Python](https://www.nltk.org/book/)
